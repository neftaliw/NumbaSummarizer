{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving performance\n",
    "\n",
    "There are different ways to improve performance of a Python program. In ICS 46 you will learn about improving an algorithm through static analysis. Today, we are going to learn about how to improve performance from the hardware's point of view. Some of the concepts that we will learn today will be revisited in further courses, but it is imperative that as a future developer you learn what tools you have at your disposal.\n",
    "\n",
    "## Hardware\n",
    "\n",
    "Let's think about your hardware. Your code will at least run in a personal computer. These have what is called an architecture, a way the components interact with each other. The architecture is usually defined by the processor (CPU), and we are a long way since we had single threaded CPUs.\n",
    "\n",
    "CPUs with multiple cores have become the standard in the recent development of modern computer architectures and we can not only find them in supercomputer facilities but also in our desktop machines at home, and our laptops; even Apple’s iPhone 5S got a 1.3 Ghz Dual-core processor in 2013.\n",
    "\n",
    "However, the default Python interpreter was designed with simplicity in mind and has a thread-safe mechanism, the so-called “GIL” (Global Interpreter Lock). In order to prevent conflicts between threads, it executes only one statement at a time (so-called serial processing, or single-threading).\n",
    "\n",
    "\n",
    "### Vectorization\n",
    "One of the properties of many multi-core processors is the ability to process multiple points of data in a single pass. There's a name for these processors and it is SIMD (Single instruction multiple data). The process of transforming a code into it's SIMD form is called vectorization. With it we use a special kind of local parallelism available in multi core processors and it is the main source of their effectiveness. \n",
    "\n",
    "The following image represents how a simple operation such as addign two numbers can be sped up using SIMD instructions. While in scalar processing, each addition would be done after the previous one, using vector registers we can load up two n numbers (where n is the size of integers a vector register can hold) and apply addition to all of them at the same time.\n",
    "\n",
    "<img src=\"images/image.png\">\n",
    "\n",
    "\n",
    "\n",
    "## Interlude: Profiling\n",
    "\n",
    "Before we go any further and start looking at how vectorization makes your program faster, we need to talk about profiling. Profiling is the act of measuring performance of a program, either by timing it or by looking into memory access, depending on what is you are trying to measure.\n",
    "\n",
    "(Follow the instructions here: https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html to setup the profilers)\n",
    "\n",
    "### Time\n",
    "\n",
    "This is the most common profiler. In a python code you just import the time module and measure starting and ending time. For IPython we can call the %time %%time and %%timeit magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 ms ± 17.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        total += i * (-1) ** j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        total += i * (-1) ** j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting an unsorted list:\n",
      "Wall time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "L = [random.random() for i in range(100000)]\n",
    "print(\"sorting an unsorted list:\")\n",
    "%time L.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting an already sorted list:\n",
      "Wall time: 2.02 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"sorting an already sorted list:\")\n",
    "%time L.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that sort() is way faster once a list is already sorted. You can see the different usage of time, timeit runs the code several times and return the statistics for all the runs.\n",
    "\n",
    "### Prun\n",
    "\n",
    "A program is made of many single statements, and sometimes timing these statements in context is more important than timing them on their own. Python contains a built-in code profiler (which you can read about in the Python documentation), but IPython offers a much more convenient way to use this profiler, in the form of the magic function %prun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_lists(N):\n",
    "    total = 0\n",
    "    for i in range(5):\n",
    "        L = [j ^ (j >> i) for j in range(N)]\n",
    "        total += sum(L)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun sum_of_lists(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a table that indicates, in order of total time on each function call, where the execution is spending the most time. In this case, the bulk of execution time is in the list comprehension inside sum_of_lists. From here, we could start thinking about what changes we might make to improve the performance in the algorithm.\n",
    "\n",
    "### Memit\n",
    "\n",
    "This profiler requires you to install memory_profiler. The memit magic allows us to measure memory performance of a given code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 134.28 MiB, increment: 75.79 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit sum_of_lists(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this function uses about 100 MB of memory.\n",
    "\n",
    "For a line-by-line description of memory use, we can use the %mprun magic. Unfortunately, this magic works only for functions defined in separate modules rather than the notebook itself, so we'll start by using the %%file magic to create a simple module called mprun_demo.py, which contains our sum_of_lists function, with one addition that will make our memory profiling results more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mprun_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file mprun_demo.py\n",
    "def sum_of_lists(N):\n",
    "    total = 0\n",
    "    for i in range(5):\n",
    "        L = [j ^ (j >> i) for j in range(N)]\n",
    "        total += sum(L)\n",
    "        del L # remove reference to L\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled.\n"
     ]
    }
   ],
   "source": [
    "from mprun_demo import sum_of_lists\n",
    "%mprun -f sum_of_lists sum_of_lists(1000000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel Distribution for Python\n",
    "\n",
    "Now that we have learned about profiling, let's go back to hardware optimization. Since Python is an interpreted language, that means there is no compiler doing code optimizations when running on a machine. However, not everything in Python is Python. Many libraries use C as a backend language. For example, Numpy, a very popular library for numerical computations, is compiled using C, and can take the advantages that a compiled language provides, so that whenever you call on the Numpy module you can run an optimized version of it. \n",
    "\n",
    "(IDP is not the only distribution that optimizes Python. look into Numba and PyPy for other examples).\n",
    "\n",
    "IDP optimizes modules by implementing optimizations such as vectorization. This allows us to run a big number of computations with a reduced running time. Let's go to the script called intelsample to see how intel is faster than typical python.\n",
    "\n",
    "### Why is it  faster\n",
    "\n",
    "Intel gets this acceleration by focusing on three things:\n",
    "\n",
    "1. Taking advantage of multicore\n",
    "2. Taking advantage of vector (also called SIMD) instructions such as SSE, AVX, AVX2, and AVX-512\n",
    "3. Using advanced algorithms in the Intel® Math Kernel Library (Intel® MKL)\n",
    "\n",
    "All three of these happen in programs that operate on vectors or matrices.  We shouldn’t expect big speed-ups for an occasional standalone cosine (that is, not in a loop cycle). Nor should we expect as much speed-up on a single core processor as on a multicore one.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The easiest way to setup IDP is through anaconda. For the instructions let's go here: https://software.intel.com/en-us/articles/intel-distribution-for-python-development-environment-setting-for-jupyter-notebook-and\n",
    "\n",
    "## Parallel computing\n",
    "\n",
    "ICS 131 deals with parallel computing in detail. You are highly encouraged to take that course even if you don't plan to graduate with a parallel computing minor.\n",
    "\n",
    "Let's talk about parallelism.\n",
    "\n",
    "### Concurrency\n",
    "\n",
    "In programming, concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.\n",
    "\n",
    "We will not look into concurrency but it is a topic usually taught along with parallelism. Though this sentence sums it up perfectly.\n",
    "\n",
    "### Parallel vs serial\n",
    "\n",
    "So far you have done single tasks step by step. This is called serial computing because there is a series of steps taken form the beginning of our code to the very end. Parallel computing is a change in paradigm where you have several process doing similar steps on different section of the data. Each process runs on their own clock and usually don't depend on the other processes\n",
    "\n",
    "<img src=\"images/image2.png\">\n",
    "\n",
    "### Threads vs process\n",
    "\n",
    "A thread is a sequence of instructions within a process. It can be thought of as a lightweight process. Threads share the same memory space.\n",
    "\n",
    "A process is an instance of a program running in a computer which can contain one or more threads. A process has its independent memory space.\n",
    "\n",
    "#### Python's GIL\n",
    "\n",
    "The CPython implementation has a Global Interpreter Lock (GIL) which allows only one thread to be active in the interpreter at once. This means that threads cannot be used for parallel execution of Python code. While parallel CPU computation is not possible, parallel IO operations are possible using threads. This is because performing IO operations releases the GIL. \n",
    "\n",
    "### Multi-threading vs Multi-processing\n",
    "\n",
    "Notice, the separation of these two concepts is almost exclusive to Python becaus of the GIL. In languages like C there's technically no difference between the two concepts\n",
    "\n",
    "Depending on the application, two common approaches in parallel programming are either to run code via threads or multiple processes, respectively. If we submit “jobs” to different threads, those jobs can be pictured as “sub-tasks” of a single process and those threads will usually have access to the same memory areas (i.e., shared memory). Another approach is to submit multiple processes to completely separate memory locations (i.e., distributed memory): Every process will run completely independent from each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image3.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads should not be used for CPU bound tasks. Using threads for CPU bound tasks will actually result in worse performance compared to using a single thread.\n",
    "\n",
    "Let me demonstrate why it’s a bad idea to use threads for CPU bound tasks. In the following program a queue holds numbers. The task is to find the sum of prime number less than or equal to the given number. This is clearly a CPU bound task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prime.py\n"
     ]
    }
   ],
   "source": [
    "%%file prime.py\n",
    "\n",
    "def sum_prime(num):\n",
    "    \n",
    "    sum_of_primes = 0\n",
    "\n",
    "    ix = 2\n",
    "\n",
    "    while ix <= num:\n",
    "        if is_prime(ix):\n",
    "            sum_of_primes += ix\n",
    "        ix += 1\n",
    "\n",
    "    return sum_of_primes\n",
    "\n",
    "def is_prime(num):\n",
    "    if num <= 1:\n",
    "        return False\n",
    "    elif num <= 3:\n",
    "        return True\n",
    "    elif num%2 == 0 or num%3 == 0:\n",
    "        return False\n",
    "    i = 5\n",
    "    while i*i <= num:\n",
    "        if num%i == 0 or num%(i+2) == 0:\n",
    "            return False\n",
    "        i += 6\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37550402023, 142913828922, 312471072265]\n",
      "Execution time = 17.40201\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "list_lock = threading.Lock()\n",
    "\n",
    "def find_rand(num):\n",
    "    sum_of_primes = 0\n",
    "\n",
    "    ix = 2\n",
    "\n",
    "    while ix <= num:\n",
    "        if is_prime(ix):\n",
    "            sum_of_primes += ix\n",
    "        ix += 1\n",
    "\n",
    "    sum_primes_list.append(sum_of_primes)\n",
    "\n",
    "def is_prime(num):\n",
    "    if num <= 1:\n",
    "        return False\n",
    "    elif num <= 3:\n",
    "        return True\n",
    "    elif num%2 == 0 or num%3 == 0:\n",
    "        return False\n",
    "    i = 5\n",
    "    while i*i <= num:\n",
    "        if num%i == 0 or num%(i+2) == 0:\n",
    "            return False\n",
    "        i += 6\n",
    "    return True\n",
    "\n",
    "def process_queue():\n",
    "    while True:\n",
    "        rand_num = min_nums.get()\n",
    "        find_rand(rand_num)\n",
    "        min_nums.task_done()\n",
    "\n",
    "min_nums = Queue()\n",
    "\n",
    "rand_list = [1000000, 2000000, 3000000]\n",
    "sum_primes_list = list()\n",
    "\n",
    "for i in range(3):\n",
    "    t = threading.Thread(target=process_queue)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for rand_num in rand_list:\n",
    "    min_nums.put(rand_num)\n",
    "\n",
    "min_nums.join()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "sum_primes_list.sort()\n",
    "print(sum_primes_list)\n",
    "\n",
    "print(\"Execution time = {0:.5f}\".format(end_time - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parallel execution of tasks, the multiprocessing module can be used.\n",
    "\n",
    "In the following example we take the same task used above and process the inputs in parallel using the multiprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "import prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37550402023, 142913828922, 312471072265]\n",
      "Time taken = 10.09113\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    with Pool(4) as p:\n",
    "        print(p.map(prime.sum_prime, [1000000, 2000000, 3000000]))\n",
    "    print(\"Time taken = {0:.5f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using the multiprocessing module results in the full utilization of the CPU.\n",
    "\n",
    "Here's another example where we are partitioning a loop into several threads. This can be done more elegantly using map function but we generate each process manually for analysis. The following loop is iterating through a two dimensional matrix, and is adding it's elements with those of another two dimensional matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsvclooppar(nstart, ntimes, LEN, a, c, listh):\n",
    "    print(\" starting\" )\n",
    "    for nl in range(nstart,ntimes-1):\n",
    "        dot=0\n",
    "        for i in listh:\n",
    "            for j in listh:\n",
    "                a[i,j]=a[i,j]+c[i,j]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " starting\n",
      "Wall time: 56.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\tLEN=500\n",
    "\tntimes=500\n",
    "\tnstart=1\n",
    "\th=list(range(1,ntimes-1))\n",
    "\tb=np.linspace(0, 500, LEN)\n",
    "\tg=[]\n",
    "\tfor iterx in range (0,LEN):\n",
    "    \t\tg.append(b)\n",
    "\n",
    "\ta=np.array([[x for x in y] for y in g])\n",
    "\n",
    "\tc=np.array([[x for x in y] for y in g])\n",
    "\ttsvclooppar(1,500,LEN,a,c,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing  \n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__=='__main__':\n",
    "\tLEN=500\n",
    "\tntimes=500\n",
    "\tnstart=1\n",
    "\th=list(range(1,ntimes-1))\n",
    "\tb=np.linspace(0, 500, LEN)\n",
    "\tg=[]\n",
    "\tfor iterx in range (0,LEN):\n",
    "    \t\tg.append(b)\n",
    "\n",
    "\ta=np.array([[x for x in y] for y in g])\n",
    "\n",
    "\tc=np.array([[x for x in y] for y in g])\n",
    "\tp1 = multiprocessing.Process(target=defs.tsvclooppar, args=(nstart,125,LEN, a,c,h,))\n",
    "\tp1.start()\n",
    "\tp2 = multiprocessing.Process(target=defs.tsvclooppar, args=(126,250,LEN, a,c,h,))\n",
    "\tp2.start()\n",
    "\tp3 = multiprocessing.Process(target=defs.tsvclooppar, args=(251,375,LEN, a,c,h,))\n",
    "\tp3.start()\n",
    "\tp4 = multiprocessing.Process(target=defs.tsvclooppar, args=(376,ntimes,LEN, a,c,h,))\n",
    "\tp4.start()\n",
    "\tp1.join()\n",
    "\tp2.join()\n",
    "\tp3.join()\n",
    "\tp4.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's so much more to learn about multiprocessing, and we will continue on it tomorrow at the lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
