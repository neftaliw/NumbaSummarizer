{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Numba\n",
    "\n",
    "Last class you learned about how wonderful parallel computing is. We also learned that your own processor can exploit the full power of local parallelism. However, when getting to the nitty gritty it gets really messy. The multiprocessing module requires you to do manual workload distribution, and it is a complex and error prone task. Enter Numba, a Just In Time (JIT) compiler for Python. Numba takes the code you wrote and compiles it to exploit hardware architecture. Before we go into the specifics you need to know what a JIT compiler is.\n",
    "\n",
    "## Compilers vs Interpreters\n",
    "\n",
    "CPython (the standard Python distribution) is an \"interpreted language\". While you should know the differences between compiled and interpreted computing (and if you don't you really need to take ICS 51), here's a short video to serve as a refresher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_C5AHaS1mOA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_C5AHaS1mOA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the advantages of compiled computation:\n",
    "- Running time is faster, this is because there's no trips between the language and the processor\n",
    "- Code can be optimized. If the compiler knows the whole application is translating, it can find better ways to run your code\n",
    "- Enforces safe, working code. Since your code needs to work in order to run, it's an all or nothing situation\n",
    "\n",
    "On the other hand, interpreters have advantages to:\n",
    "- Faster initial deployment, there's no need to look at the whole time is a \"pay as you go\" mode.\n",
    "- Easier debugging. Most problems will arise right after running the problematic instruction.\n",
    "- Allows for dynamic coding, where a programmer can change code without having to recompile the whole thing\n",
    "\n",
    "But, what if we could exploit the advantages of the two, hence JIT compilers.\n",
    "\n",
    "## JIT\n",
    "\n",
    "JIT compilers run as an interpreter but instead of going line by line, it chooses block of code (usually functions) and compiles them, runs them and then comes back to the next block of code. This makes JIT generally faster than interpreters, but it also allows for applying local optimizations to the blocks of code being compiled.\n",
    "\n",
    "Numba is a JIT compiler for Python (name comes from Numpy +  Mamba which is a very fast snake). It take in Python code, spearates it by compilable blocks and sends it to LLVM which is the backend compiler.\n",
    "\n",
    "Because of it's nature, Numba works best with optimized distributions of Numpy. \n",
    "\n",
    "Numba is not included with Python so you will need to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\nefta\\anaconda3\\lib\\site-packages (0.43.1)\n",
      "Requirement already satisfied: llvmlite>=0.28.0dev0 in c:\\users\\nefta\\anaconda3\\lib\\site-packages (from numba) (0.28.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nefta\\anaconda3\\lib\\site-packages (from numba) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of how numba optimizes code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "x = np.arange(1000000).reshape(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def go_slow(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "go_slow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 310 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def go_fast(a): # Function is compiled to machine code when called the first time\n",
    "    trace = 0\n",
    "    for i in range(a.shape[0]):   # Numba likes loops\n",
    "        trace += np.tanh(a[i, i]) # Numba likes NumPy functions\n",
    "    return a + trace              # Numba likes NumPy broadcasting\n",
    "\n",
    "go_fast(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et Voilà!!!.... wait a minute, that is not faster. In fact it is about 50 times slower, what is going on?. Simple, we are timing the function's declaration along with it's instantiation. Therefore, what we are seeing there is the time it took to compile the function. Let's run it again, but now we are only running the instantiation. We'll use timeit to get more accurate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 ms ± 428 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "go_slow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.57 ms ± 250 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "go_fast(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, about half the running time (I know is not that impressive but this loop uses a numpy function, so even the interpreted version is already optimized. Let's go ahead and look into how to use the numba decorators:\n",
    "\n",
    "## Nopython mode\n",
    "\n",
    "Numba operates through the @jit decorator. This can take arguments, one of them is enabling the nopython mode. This means that it will not let the Python interpreter to have any involvement in the compilation. When not set as True (which is the default behavior) then it runs in object-mode, which means that if compiling fails (as in the case where we are using libraries unknown to Numba), it will fall back to Python's default interpreter.\n",
    "\n",
    "Numba is an incredibly sophisticated tool that includes decorator for the use with stencil computation, GPU programming and more. For this class we are going to focus on the basic functionality to optimize your code and to exploit local parallelism.\n",
    "\n",
    "## Nogil mode\n",
    "\n",
    "Setting the nogil mode will allow for running native Python but with the GIL released. This allows for multi-threding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@jit(nogil=True)\n",
    "def f(x, y):\n",
    "    return x + y\n",
    "f(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inlining functions\n",
    "\n",
    "One of the major optimizations that Numba does is inlining. This means that compatible functions that do similar operations can run in parallel using vector instructions, we'll look into a more detailed example later on but oftentimes inlining will be faster than running the functions separately. In the following example, hypot is calling upon the function square twice. See how it is faster inside the jit compiled function than doing the same operation separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "@jit\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    return math.sqrt(square(x) + square(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 ns ± 9.19 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hypot(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 ns ± 23 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "math.sqrt(square(3) + square(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel mode\n",
    "\n",
    "The parallel mode allows for automatic parallelism of your code. This will work with specific instructions, most ppular ones are:\n",
    "- unary operators: + - ~\n",
    "- binary operators: + - * / /? % | >> ^ << & ** //\n",
    "- comparison operators: == != < <= > >=\n",
    "- reduce()\n",
    "- Numpy array functions like zeros, ones, arange, linspace, dot, sum, prod, min, max, argmin, argmax, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@jit(nopython=True, parallel=True)\n",
    "def f(x, y):\n",
    "    return x + y\n",
    "f(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit parallelism\n",
    "\n",
    "Sometimes a loop is \"embarassingly\" parallel but the parallel argument still won't yield better performance. For those cases we can use loops that are explicitly parallel. This is done through Numba's own prgane. However you need to ensure that there is no cross iteration dependencies (more on that topic later on). Here's an example with a one dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def prange_test(A):\n",
    "    s = 0\n",
    "    # Without \"parallel=True\" in the jit-decorator\n",
    "    # the prange statement is equivalent to range\n",
    "    for i in prange(A.shape[0]):\n",
    "        s += A[i]\n",
    "    return s\n",
    "def range_test(A):\n",
    "    s=0\n",
    "    for i in range(A.shape[0]):\n",
    "        s += A[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24267202 0.90299416 0.68211224 ... 0.92767258 0.53343004 0.27722563]\n"
     ]
    }
   ],
   "source": [
    "A=np.random.rand(10000)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38 ms ± 47.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "range_test(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7 µs ± 591 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "prange_test(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, see what happens if there is cross iteration dependence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def prange_dep_test(A):\n",
    "    s = 0\n",
    "    B=np.copy(A)\n",
    "    # Without \"parallel=True\" in the jit-decorator\n",
    "    # the prange statement is equivalent to range\n",
    "    for i in prange(1,B.shape[0]-1):\n",
    "        B[i]=B[i]+B[i+1]*B[i-1]\n",
    "        s += B[i]\n",
    "    return s\n",
    "def range_dep_test(A):\n",
    "    s=0\n",
    "    B=np.copy(A)\n",
    "    for i in range(1,B.shape[0]-1):\n",
    "        B[i]=B[i]+B[i+1]*B[i-1]\n",
    "        s += B[i]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740.0015226632324"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prange_dep_test(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974.3861328097464"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_dep_test(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now they are not the same, that's because of the cross dependence, so you have to be very careful when you are using prange.\n",
    "\n",
    "## Final words\n",
    "\n",
    "There is a lot more you can do with Numba, and as always the official documentation is your first point of reference (https://numba.pydata.org/numba-doc/latest/index.html). We are going to keep playing with this tool in our next session, but in the meantime get familiarized with it, try to think of opportunities to use it in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
